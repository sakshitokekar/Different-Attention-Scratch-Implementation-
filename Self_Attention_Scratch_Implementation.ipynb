{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Normal Implementation"
      ],
      "metadata": {
        "id": "AypNrrEfQlEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Hello World I am an attention seeker I love attention\""
      ],
      "metadata": {
        "id": "OLZgjQ0MpHRn"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dic_words = {s.lower():i for i,s in enumerate(sorted(prompt.split(\" \")))}"
      ],
      "metadata": {
        "id": "QJpoVTTsErKc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dic_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GG54FBnE2Oq",
        "outputId": "97c93da9-36dc-4b5e-90e7-e9ebbdacdfef"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'hello': 0, 'i': 2, 'world': 3, 'am': 4, 'an': 5, 'attention': 7, 'love': 8, 'seeker': 9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [dic_words[i.lower()] for i in prompt.split(\" \")]\n",
        "print(tokens)\n",
        "\n",
        "import torch\n",
        "tokens_tf = torch.tensor(tokens)\n",
        "print(tokens_tf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQ3xxRCqFs5c",
        "outputId": "9fdbbd39-6406-4ef2-f0a4-e2c5f55088c7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 3, 2, 4, 5, 7, 9, 2, 8, 7]\n",
            "tensor([0, 3, 2, 4, 5, 7, 9, 2, 8, 7])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import Embedding\n",
        "\n",
        "torch.manual_seed(123)\n",
        "vocab_size = 50000\n",
        "embedder = Embedding(vocab_size, 3)"
      ],
      "metadata": {
        "id": "Q3BlgQtuHCRA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_embedding = embedder(tokens_tf).detach()\n",
        "print(token_embedding)\n",
        "print(token_embedding.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLL6vP5HHNK2",
        "outputId": "0744e99b-500d-4f94-b5d9-de9884e21050"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.3374, -0.1778, -0.3035],\n",
            "        [-1.1925,  0.6984, -1.4097],\n",
            "        [-0.2196, -0.3792,  0.7671],\n",
            "        [ 0.1794,  1.8951,  0.4954],\n",
            "        [ 0.2692, -0.0770, -1.0205],\n",
            "        [ 1.3010,  1.2753, -0.2010],\n",
            "        [-1.1481, -1.1589,  0.3255],\n",
            "        [-0.2196, -0.3792,  0.7671],\n",
            "        [ 0.4965, -1.5723,  0.9666],\n",
            "        [ 1.3010,  1.2753, -0.2010]])\n",
            "torch.Size([10, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d = token_embedding.shape[1]\n",
        "d_q, d_k, d_v = 24, 24, 28"
      ],
      "metadata": {
        "id": "LIpap1IyK4ap"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w_query = torch.nn.Parameter(torch.rand(d,d_q))\n",
        "w_key = torch.nn.Parameter(torch.rand(d,d_k))\n",
        "w_value = torch.nn.Parameter(torch.rand(d,d_v))"
      ],
      "metadata": {
        "id": "qO1i6rL6LGiB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = token_embedding @ w_query\n",
        "key = token_embedding @ w_key\n",
        "value = token_embedding @ w_value"
      ],
      "metadata": {
        "id": "0AL3lXi1LqjQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from torch.nn.functional import softmax\n",
        "q_kt = query @ key.T\n",
        "f = q_kt/math.sqrt(d_k)\n",
        "s = softmax(f, dim = -1)\n",
        "context_vector = s @ value"
      ],
      "metadata": {
        "id": "NmE-8-gGL5Ee"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#If you want detailed explanation on code expand below:"
      ],
      "metadata": {
        "id": "tmqLMB05D00O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Map each word with a value"
      ],
      "metadata": {
        "id": "7xKt9gSLARjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_map = {s.lower(): i for i, s in enumerate(sorted(prompt.split(\" \")))}\n",
        "print(word_map)"
      ],
      "metadata": {
        "id": "BavGeeuYwZxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sentence embedding"
      ],
      "metadata": {
        "id": "8t6336i1AL7d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_int = [word_map[i.lower()] for i in prompt.split(\" \")]\n",
        "print(tokens_int)"
      ],
      "metadata": {
        "id": "OwtlEDfb7v3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "tokens_tf = torch.tensor(tokens_int)\n",
        "print(tokens_tf)"
      ],
      "metadata": {
        "id": "0IeiMsplA9cQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Self Attention mechanism works on vectors and not on strings or int indices. Hence we need to convert to vectors\n"
      ],
      "metadata": {
        "id": "QQ4M0u4qO-ki"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Use of manual_seed(123)"
      ],
      "metadata": {
        "id": "wmb2FOckX1_L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The `torch.manual_seed(123)` function is used to set the **random seed** for PyTorch's random number generator. Let’s break down what this means and why it’s important.\n",
        "\n",
        "---\n",
        "\n",
        "##### **What Does `123` Indicate?**\n",
        "- The number `123` is an arbitrary integer value that you choose as the **seed** for the random number generator.\n",
        "- This seed initializes the random number generator, ensuring that any random operations in your code (e.g., weight initialization in neural networks, random shuffling of data, etc.) produce the **same results every time** you run the code.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Why Do We Use `torch.manual_seed(123)`?**\n",
        "\n",
        "##### 1. **Reproducibility**\n",
        "- In machine learning and deep learning, many operations involve randomness, such as:\n",
        "  - Initializing weights in a neural network.\n",
        "  - Shuffling data during training.\n",
        "  - Dropout layers.\n",
        "- Without setting a random seed, these operations would produce different results each time you run the code, making it difficult to reproduce experiments or debug issues.\n",
        "- By setting a fixed seed (e.g., `123`), you ensure that the random numbers generated are the same every time you run the code. This makes your experiments **reproducible**.\n",
        "\n",
        "##### 2. **Debugging**\n",
        "- When debugging a model, you want to ensure that the behavior is consistent across runs. If the results change every time you run the code, it becomes difficult to identify whether a change in behavior is due to a bug or randomness.\n",
        "- Setting a random seed eliminates this variability, making debugging easier.\n",
        "\n",
        "##### 3. **Fair Comparisons**\n",
        "- When comparing different models or hyperparameters, you want to ensure that the comparison is fair and not influenced by random initialization or other stochastic factors.\n",
        "- By using the same random seed, you ensure that all models start from the same initial conditions, making the comparison meaningful.\n",
        "\n",
        "---\n",
        "\n",
        "#### **How Does It Work?**\n",
        "- When you call `torch.manual_seed(123)`, PyTorch’s random number generator is initialized with the seed value `123`.\n",
        "- Any subsequent random operations (e.g., `torch.rand()`, `nn.Embedding` initialization, etc.) will produce the same sequence of random numbers every time you run the code.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Example**\n",
        "\n",
        "##### Without Setting a Seed\n",
        "```python\n",
        "import torch\n",
        "\n",
        "# Random tensor without setting a seed\n",
        "print(torch.rand(3))  # Output will be different every time\n",
        "```\n",
        "\n",
        "Output (Run 1):\n",
        "```\n",
        "tensor([0.1234, 0.5678, 0.9101])\n",
        "```\n",
        "\n",
        "Output (Run 2):\n",
        "```\n",
        "tensor([0.4321, 0.8765, 0.1098])\n",
        "```\n",
        "\n",
        "- The output is different each time because the random number generator is initialized with a different seed.\n",
        "\n",
        "---\n",
        "\n",
        "##### With Setting a Seed\n",
        "```python\n",
        "import torch\n",
        "\n",
        "# Set the random seed\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# Random tensor with a fixed seed\n",
        "print(torch.rand(3))  # Output will be the same every time\n",
        "```\n",
        "\n",
        "Output (Run 1):\n",
        "```\n",
        "tensor([0.2961, 0.5166, 0.2517])\n",
        "```\n",
        "\n",
        "Output (Run 2):\n",
        "```\n",
        "tensor([0.2961, 0.5166, 0.2517])\n",
        "```\n",
        "\n",
        "- The output is the same every time because the random number generator is initialized with the same seed (`123`).\n",
        "\n",
        "---\n",
        "\n",
        "#### **Why Use `123` Specifically?**\n",
        "- The value `123` is arbitrary. You can use any integer value as the seed (e.g., `42`, `100`, `999`, etc.).\n",
        "- The choice of seed does not affect the quality of randomness; it only ensures reproducibility.\n",
        "- Commonly used seeds include `42` (a popular choice in the machine learning community) or `123` (as in this example).\n",
        "\n",
        "---\n",
        "\n",
        "#### **When to Use `torch.manual_seed`?**\n",
        "You should use `torch.manual_seed` in the following scenarios:\n",
        "1. **Weight Initialization**:\n",
        "   - When initializing the weights of a neural network, you want to ensure that the initialization is consistent across runs.\n",
        "   ```python\n",
        "   torch.manual_seed(123)\n",
        "   model = nn.Linear(10, 1)  # Weights will be the same every time\n",
        "   ```\n",
        "\n",
        "2. **Data Shuffling**:\n",
        "   - When shuffling data during training, you want to ensure that the order of the data is consistent across runs.\n",
        "   ```python\n",
        "   torch.manual_seed(123)\n",
        "   indices = torch.randperm(100)  # Same shuffle every time\n",
        "   ```\n",
        "\n",
        "3. **Dropout Layers**:\n",
        "   - Dropout layers use randomness to deactivate neurons during training. Setting a seed ensures that the same neurons are deactivated every time.\n",
        "   ```python\n",
        "   torch.manual_seed(123)\n",
        "   dropout = nn.Dropout(0.5)\n",
        "   ```\n",
        "\n",
        "4. **Embedding Layers**:\n",
        "   - Embedding layers use random initialization for their weights. Setting a seed ensures that the embeddings are initialized consistently.\n",
        "   ```python\n",
        "   torch.manual_seed(123)\n",
        "   embed = nn.Embedding(100, 10)  # Same embeddings every time\n",
        "   ```\n",
        "\n",
        "---\n",
        "\n",
        "#### **Summary**\n",
        "- `torch.manual_seed(123)` sets the random seed for PyTorch’s random number generator to `123`.\n",
        "- This ensures that any random operations in your code produce the same results every time you run it, which is crucial for **reproducibility**, **debugging**, and **fair comparisons**.\n",
        "- The value `123` is arbitrary; you can use any integer value as the seed."
      ],
      "metadata": {
        "id": "kpQKFWZxWSjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)"
      ],
      "metadata": {
        "id": "yZjwAH2NCXUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Setting a vocabulary size"
      ],
      "metadata": {
        "id": "Nw0-TeQbYFYU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the total number of unique words in the vocabulary. Here, we assume a large vocabulary size of 50,000 words."
      ],
      "metadata": {
        "id": "YqOWTDCkYM4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 50000"
      ],
      "metadata": {
        "id": "1RvLjtSSYFB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Creating an Embedding Layer"
      ],
      "metadata": {
        "id": "In5xZCg3YONt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **What is an embedding layer?\n",
        "\n",
        "  - An embedding layer is a lookup table that maps integer indices (representing words) to dense vectors of fixed size (embeddings).\n",
        "\n",
        "  - Each word in the vocabulary is represented by a vector of size 3 (in this case).\n",
        "\n",
        "\n",
        "\n",
        "* **Parameters:**\n",
        "\n",
        "  - vocab_size: The size of the vocabulary (50,000 in this case).\n",
        "\n",
        "  - 3: The dimensionality of the embedding vectors. Each word will be represented by a 3-dimensional vector.\n",
        "\n",
        "\n",
        "\n",
        "* **How it works:**\n",
        "\n",
        "  - The embedding layer is essentially a matrix of size (vocab_size, embedding_dim). For example, if vocab_size = 50000 and embedding_dim = 3, the embedding layer is a matrix of size (50000, 3).\n",
        "\n",
        "  - When you pass an integer index to the embedding layer, it looks up the corresponding row in this matrix and returns the embedding vector."
      ],
      "metadata": {
        "id": "0PVudyawYiS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "embed = nn.Embedding(vocab_size, embedding_dim=3)"
      ],
      "metadata": {
        "id": "CzJx7D-pYJdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Embedding the tokenized int indices"
      ],
      "metadata": {
        "id": "ynuiI3v-Zv6Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Input:**\n",
        "\n",
        "  - tokens_tf: A tensor of integer indices representing the sentence.\n",
        "\n",
        "* **What happens here?**\n",
        "\n",
        "  - The embedding layer (embed) takes the integer indices and maps each index to its corresponding embedding vector.\n",
        "\n",
        "  - For example:\n",
        "\n",
        "    - Index 0 -> Embedding vector [0.1, 0.2, 0.3]\n",
        "\n",
        "    - Index 8 -> Embedding vector [0.4, 0.5, 0.6]\n",
        "\n",
        "    - And so on.\n",
        "\n",
        "* **Output:**\n",
        "\n",
        "  - The output is a tensor of shape (sequence_length, embedding_dim), where:\n",
        "\n",
        "  - sequence_length is the number of words in the sentence (9 in this case).\n",
        "\n",
        "  - embedding_dim is the dimensionality of the embedding vectors (3 in this case).\n",
        "\n",
        "* **Why .detach()?**\n",
        "\n",
        "  - The .detach() method is used to remove the tensor from the computation graph. This is done to ensure that the tensor is treated as a constant and does not participate in gradient calculations during backpropagation. This is often used when you want to use the embedded vectors as input to another part of the model without affecting the gradients. This makes detach() particularly useful in scenarios where you want to avoid unnecessary memory usage or stop gradient calculations on specific parts of your tensor.\n",
        "\n",
        "  - https://medium.com/biased-algorithms/understanding-tensor-detach-in-pytorch-a-practical-guide-e859a7713f28#:~:text=detach()%20is%20invaluable%20when,memory%20optimization%20in%20complex%20models.\n",
        "\n"
      ],
      "metadata": {
        "id": "RpladoSTZjPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_sentence = embed(tokens_tf).detach()"
      ],
      "metadata": {
        "id": "xmoMR2XcZi87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedded_sentence)\n",
        "print(embedded_sentence.shape)"
      ],
      "metadata": {
        "id": "uYQ-bogGa6r7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Defining Weight Matrices"
      ],
      "metadata": {
        "id": "S-1e4pTrxcl_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The embedding dimension (d) is required to define the size of the weight matrices (W_query, W_key, W_value) for the self-attention mechanism."
      ],
      "metadata": {
        "id": "uHvOoMquwXfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d = embedded_sentence.shape[1]"
      ],
      "metadata": {
        "id": "QkQXUkhlwXGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Purpose:**\n",
        "  - These variables define the dimensionality of the query, key, and value vectors in the self-attention mechanism.\n",
        "\n",
        "**What do these represent?:**\n",
        "\n",
        "  - d_q: Dimensionality of the query vectors.\n",
        "\n",
        "  - d_k: Dimensionality of the key vectors.\n",
        "\n",
        "  - d_v: Dimensionality of the value vectors.\n",
        "\n",
        "**Why are they different?:**\n",
        "\n",
        "  - In some self-attention implementations, the dimensionality of queries, keys, and values can be different. However, in many cases (e.g., Transformer models), d_q and d_k are the same to allow for dot-product attention.\n",
        "\n",
        "  - Here, d_q = 24, d_k = 24, and d_v = 28 are arbitrary choices for demonstration."
      ],
      "metadata": {
        "id": "ZrfoLOuIxH-Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(It’s important to note that d\n",
        " represents the size of each word vector, x\n",
        ".)\n",
        "\n",
        "Since we are computing the dot-product between the query and key vectors, these two vectors have to contain the same number of elements (dq=dk\n",
        "). However, the number of elements in the value vector v(i)\n",
        ", which determines the size of the resulting context vector, is arbitrary."
      ],
      "metadata": {
        "id": "p5PblYYMy_BO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_q, d_k, d_v = 24, 24, 28"
      ],
      "metadata": {
        "id": "14suVcLwiUnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weight Matrices for Self-Attention"
      ],
      "metadata": {
        "id": "aJVisehmzKfY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. torch.rand(d_q, d):\n",
        "\n",
        "  - Generates a random tensor of shape (d_q, d) with values sampled from a uniform distribution between 0 and 1.\n",
        "\n",
        "  - For example, if d_q = 24 and d = 3, this creates a tensor of shape (24, 3).\n",
        "\n",
        "2. torch.nn.Parameter(...):\n",
        "\n",
        "  - In neural networks, certain tensors (e.g., weights of layers) need to be updated during training to minimize the loss function. These tensors are called learnable parameters.\n",
        "\n",
        "  - By wrapping a tensor with torch.nn.Parameter, we tell PyTorch that this tensor should be updated during training via backpropagation.\n",
        "\n",
        "  - Wraps the tensor as a learnable parameter. This means that during training, the values of these tensors will be updated via backpropagation.\n",
        "\n",
        "  - Parameters are automatically tracked by PyTorch for gradient computation.\n",
        "\n",
        "  - These weight matrices are learnable parameters that will be updated during training.\n",
        "\n",
        "  - During the forward pass, they are used to project the input embeddings into query, key, and value vectors.\n",
        "\n",
        "  - During backpropagation, PyTorch computes gradients for these matrices and updates their values to minimize the loss.\n",
        "\n",
        "\n",
        "3. W_query, W_key, W_value:\n",
        "\n",
        "  - These are the weight matrices for the query, key, and value projections, respectively.\n",
        "\n",
        "  - Their shapes are:\n",
        "\n",
        "    - W_query: (d_q, d)\n",
        "\n",
        "    - W_key: (d_k, d)\n",
        "\n",
        "    - W_value: (d_v, d)"
      ],
      "metadata": {
        "id": "cdsvj33H1_Ug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w_query = torch.nn.Parameter(torch.rand(d,d_q))\n",
        "w_key = torch.nn.Parameter(torch.rand(d,d_k))\n",
        "w_value = torch.nn.Parameter(torch.rand(d,d_v))"
      ],
      "metadata": {
        "id": "IKTDJl8PzDj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing unnormalized attention weights"
      ],
      "metadata": {
        "id": "koqU1Q72x0xs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = embedded_sentence @ w_query\n",
        "keys = embedded_sentence @ w_key\n",
        "values = embedded_sentence @ w_value"
      ],
      "metadata": {
        "id": "OY6wVEIs-K-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention Formula\n",
        "\n",
        "####$\\text{Attention}(Q, K, V) = \\text{SoftMax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$."
      ],
      "metadata": {
        "id": "eVQQFbxzBIWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_kt = query @ keys.T"
      ],
      "metadata": {
        "id": "vZClqufIA9mN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "f = q_kt / math.sqrt(d_k)"
      ],
      "metadata": {
        "id": "-piLEhJ2CBjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Softmax"
      ],
      "metadata": {
        "id": "HAOB2J7BCaQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "s = F.softmax(f, dim = -1)"
      ],
      "metadata": {
        "id": "PWZFHps5CbgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_vector = s @ values\n",
        "print(context_vector)"
      ],
      "metadata": {
        "id": "O85fncWgDj9o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}