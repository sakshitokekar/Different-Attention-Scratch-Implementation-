{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The masked attention formula is given by: $\\text{MaskedAttention}(Q, K, V, M) = \\text{SoftMax}\\left(\\frac{QK^T}{\\sqrt{d_k}} + M\\right)V$."
      ],
      "metadata": {
        "id": "_fF1agIeuRGw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rq0TXHn9TbJ_"
      },
      "outputs": [],
      "source": [
        " import torch\n",
        " from torch.nn import Embedding\n",
        "\n",
        " def token_vectorization(prompt):\n",
        "  word_dic = {s.lower():i for i,s in enumerate(sorted(prompt.split(\" \")))}\n",
        "  tokens_int = [word_dic[i.lower()] for i in prompt.split(\" \")]\n",
        "  tokens_tf = torch.tensor(tokens_int)\n",
        "  vocab_size = 50000\n",
        "  torch.manual_seed(123)\n",
        "  embedder = Embedding(vocab_size, 3)\n",
        "  token_embedding = embedder(tokens_tf).detach()\n",
        "  return token_embedding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_weights(token_embedding):\n",
        "  torch.manual_seed(123)\n",
        "  d = token_embedding.shape[1]\n",
        "  d_q, d_k, d_v = 24, 24, 28\n",
        "  w_query = torch.nn.Parameter(torch.rand(d,d_q))\n",
        "  w_key = torch.nn.Parameter(torch.rand(d,d_k))\n",
        "  w_value = torch.nn.Parameter(torch.rand(d,d_v))\n",
        "  q = token_embedding @ w_query\n",
        "  k = token_embedding @ w_key\n",
        "  v = token_embedding @ w_value\n",
        "  return q, k, v, d_k"
      ],
      "metadata": {
        "id": "kF36Z3eMerbD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_matrix(dim):\n",
        "  # Step 1: Create a lower triangular matrix\n",
        "  lower_triangular = torch.tril(torch.ones(dim, dim))\n",
        "  # Step 2: Invert to create an upper triangular matrix\n",
        "  upper_triangular = 1 - lower_triangular\n",
        "  # Step 3: Replace 1s with -inf\n",
        "  m = upper_triangular.masked_fill(upper_triangular == 1, float(\"-inf\"))\n",
        "  return m"
      ],
      "metadata": {
        "id": "oFBsvdnRsYo8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from torch.nn.functional import softmax\n",
        "\n",
        "def calc_attention(q,k,v,d_k,token_embedding):\n",
        "  q_kt = q @ k.T\n",
        "  f = q_kt/math.sqrt(d_k)\n",
        "  m = mask_matrix(f.shape[1])\n",
        "  f1 = f + m\n",
        "  s = softmax(f1, dim=-1)\n",
        "  context_vector = s @ v\n",
        "  return context_vector"
      ],
      "metadata": {
        "id": "Z_eX-btMes15"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = input()\n",
        "token_embedding = token_vectorization(prompt)\n",
        "q, k, v, d_k  = initialize_weights(token_embedding)\n",
        "calc_attention(q,k,v,d_k, token_embedding)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyjBdEyYXReL",
        "outputId": "a9741803-58b7-43bd-d1f5-35daeeb6f262"
      },
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello World I am an attention seeker I love attention\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0031, -0.1732, -0.1618,  0.0456,  0.0057, -0.1686, -0.2620, -0.1475,\n",
              "         -0.0319, -0.0568, -0.0197,  0.1504, -0.1717,  0.0673, -0.2114,  0.0125,\n",
              "         -0.1724,  0.2598, -0.1221,  0.1350,  0.1590,  0.1032, -0.0372,  0.1667,\n",
              "         -0.0777,  0.2618, -0.0834, -0.0971],\n",
              "        [-0.8196, -0.6501, -1.0132, -0.9124, -0.2872, -0.6141, -0.2721, -0.6837,\n",
              "         -0.8824, -1.7377, -0.7412, -1.5834, -1.1017, -0.6477, -0.7898, -0.9227,\n",
              "         -1.1314, -1.4216, -0.8238, -1.7673, -0.7232, -0.6134, -1.5221, -1.3918,\n",
              "         -1.6002, -0.8852, -0.2614,  0.1580],\n",
              "        [-0.2711, -0.2106, -0.2155, -0.2527, -0.0690, -0.1550, -0.1670, -0.2228,\n",
              "         -0.2738, -0.4673, -0.3073, -0.3992, -0.2247, -0.2208, -0.1635, -0.3456,\n",
              "         -0.3649, -0.3816, -0.1677, -0.4282, -0.3239, -0.3247, -0.3502, -0.3985,\n",
              "         -0.4569, -0.2914, -0.2056, -0.0531],\n",
              "        [ 1.2176,  1.4388,  0.6619,  0.6606,  0.1180,  0.8869,  2.0562,  1.4213,\n",
              "          1.1974,  1.5436,  1.8411,  0.4929,  0.5993,  0.8214,  0.7262,  1.7701,\n",
              "          2.0917,  0.3179,  0.4261,  0.4583,  1.5186,  1.9397,  0.6580,  0.8556,\n",
              "          1.7933,  0.4883,  1.9613,  1.2328],\n",
              "        [-0.5521, -0.5190, -0.7474, -0.5870, -0.1879, -0.4887, -0.3144, -0.5293,\n",
              "         -0.6065, -1.1852, -0.5128, -0.9813, -0.8098, -0.4037, -0.6228, -0.6175,\n",
              "         -0.8415, -0.8236, -0.6025, -1.1094, -0.4175, -0.3736, -1.0270, -0.8502,\n",
              "         -1.1058, -0.4699, -0.2258,  0.0507],\n",
              "        [ 1.2481,  1.2894,  0.5439,  0.7465,  0.1367,  0.7426,  1.7974,  1.2993,\n",
              "          1.2018,  1.5616,  1.8487,  0.7162,  0.4757,  0.9162,  0.5476,  1.8189,\n",
              "          1.9630,  0.6444,  0.3401,  0.6749,  1.7064,  2.0646,  0.6891,  1.0850,\n",
              "          1.7828,  0.7911,  1.8820,  1.1229],\n",
              "        [-0.9625, -0.7438, -0.6035, -0.8291, -0.2095, -0.4864, -0.7016, -0.7880,\n",
              "         -0.9475, -1.5126, -1.1759, -1.2449, -0.6085, -0.7919, -0.4491, -1.2821,\n",
              "         -1.2849, -1.2299, -0.4536, -1.3045, -1.2636, -1.3183, -1.0340, -1.3279,\n",
              "         -1.5237, -1.0292, -0.8939, -0.3340],\n",
              "        [ 0.3782,  0.4152,  0.1495,  0.2026,  0.0319,  0.2345,  0.6097,  0.4147,\n",
              "          0.3626,  0.4455,  0.5833,  0.1523,  0.1236,  0.2692,  0.1654,  0.5621,\n",
              "          0.6174,  0.1258,  0.0867,  0.1316,  0.5204,  0.6498,  0.1632,  0.2823,\n",
              "          0.5274,  0.1990,  0.6221,  0.3898],\n",
              "        [-0.4234, -0.3334, -0.2934, -0.3727, -0.0969, -0.2284, -0.3020, -0.3518,\n",
              "         -0.4212, -0.6877, -0.5059, -0.5667, -0.2998, -0.3449, -0.2229, -0.5558,\n",
              "         -0.5723, -0.5498, -0.2234, -0.5995, -0.5345, -0.5532, -0.4853, -0.5907,\n",
              "         -0.6865, -0.4447, -0.3729, -0.1297],\n",
              "        [ 1.2626,  1.2172,  0.4871,  0.7878,  0.1457,  0.6730,  1.6725,  1.2404,\n",
              "          1.2039,  1.5702,  1.8522,  0.8239,  0.4163,  0.9618,  0.4616,  1.8421,\n",
              "          1.9008,  0.8016,  0.2987,  0.7793,  1.7966,  2.1244,  0.7042,  1.1954,\n",
              "          1.7777,  0.9369,  1.8434,  1.0698]], grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}